{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMVLpoB6Gx8sdWxr50gPT3J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vprobon/BERC/blob/main/BERC_BMSM_wordcloud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bd4c1f4"
      },
      "source": [
        "# BERC_BMSM_wordcloud\n",
        "## Purpose: Generate a word cloud from the text extracted from a list of URLs.\n",
        "## Author: Vasilis J Promponas\n",
        "## Contact: promponas.vasileios@ucy.ac.cy\n",
        "## Date: 28/11/2025\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1c5dbdf"
      },
      "source": [
        "## Scrape Text from URLs\n",
        "\n",
        "### Subtask:\n",
        "Implement code to fetch content from each URL and extract the main text using a library like BeautifulSoup.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "690deaf5"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to import the necessary libraries: `requests` for fetching URL content, `BeautifulSoup` from `bs4` for parsing HTML, `nltk` for tokenization, lemmatization etc, and `zipfile` for ... ehm, zipping files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import tokenize\n",
        "from nltk import stem\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jRDkJTGGz3X",
        "outputId": "c5052ea5-1a36-4985-cc1c-5107675abcf0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "NcBCOc0Q2Edm"
      },
      "outputs": [],
      "source": [
        "urls_fpas = ['https://berc.ucy.ac.cy/index.php/agapios-agapiou',\n",
        "        'https://berc.ucy.ac.cy/index.php/georgios-archontis',\n",
        "        'https://berc.ucy.ac.cy/index.php/chris-christodoulou',\n",
        "        'https://berc.ucy.ac.cy/index.php/savvas-n-georgiades',\n",
        "        'https://berc.ucy.ac.cy/index.php/georgios-georgiou',\n",
        "        #'https://berc.ucy.ac.cy/index.php/antonis-kakas',\n",
        "        'https://berc.ucy.ac.cy/index.php/elpida-keravnou',\n",
        "        #'https://berc.ucy.ac.cy/index.php/constantinos-pattichis',\n",
        "        'https://berc.ucy.ac.cy/index.php/vasilis-promponas',\n",
        "        #'https://berc.ucy.ac.cy/index.php/christos-schizas',\n",
        "        ]\n",
        "urls_feng=[#'https://berc.ucy.ac.cy/index.php/chrysafis-andreou',\n",
        "        #'https://berc.ucy.ac.cy/index.php/eftychios-christoforou',\n",
        "        #'https://berc.ucy.ac.cy/index.php/julius-georgiou',\n",
        "        #'https://berc.ucy.ac.cy/index.php/theodora-krasia',\n",
        "        'https://berc.ucy.ac.cy/index.php/loucas-louca',\n",
        "        'https://berc.ucy.ac.cy/index.php/fotios-mpekris',\n",
        "        #'https://berc.ucy.ac.cy/index.php/costas-pitris',\n",
        "        'https://berc.ucy.ac.cy/index.php/triantafyllos-stylianopoulos',\n",
        "        'https://berc.ucy.ac.cy/index.php/vasileios-vavourakis',\n",
        "        ]\n",
        "urls_med =[\n",
        "        #'https://berc.ucy.ac.cy/index.php/artemios-artemiadis',\n",
        "        #'https://berc.ucy.ac.cy/index.php/panagiotis-bargiotas',\n",
        "        #'https://berc.ucy.ac.cy/index.php/anastasia-constantinidou',\n",
        "        #'https://berc.ucy.ac.cy/index.php/nikolas-dietis',\n",
        "        #'https://berc.ucy.ac.cy/index.php/georgios-hadjigeorgiou',\n",
        "        #'https://berc.ucy.ac.cy/index.php/nicos-mitsides',\n",
        "        #'https://berc.ucy.ac.cy/index.php/ilias-nikas',\n",
        "        'https://berc.ucy.ac.cy/index.php/georgios-nikolopoulos',\n",
        "        'https://berc.ucy.ac.cy/index.php/panagiotis-zis',\n",
        "        ]\n",
        "\n",
        "urls_sse =[\n",
        "      #'https://berc.ucy.ac.cy/index.php/andria-shimi'\n",
        "]\n",
        "\n",
        "\n",
        "faculty_urls = {\n",
        "    'fpas': urls_fpas,\n",
        "    'feng': urls_feng,\n",
        "    'med': urls_med,\n",
        "    #'sse': urls_sse,\n",
        "    #'all': urls_fpas + urls_feng + urls_med,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "389dcc72"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to import the `requests` library for making HTTP requests and `BeautifulSoup` from `bs4` for parsing HTML, as specified in the instructions. Then, I will initialize an empty list to store the scraped texts and loop through the provided URLs to fetch and parse their content, extracting text from paragraph tags.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "for faculty, urls in faculty_urls.items():\n",
        "  scraped_texts = []\n",
        "  for url in urls:\n",
        "      response = requests.get(url)\n",
        "      soup = BeautifulSoup(response.content, 'html.parser')\n",
        "      paragraphs = soup.find_all('p')\n",
        "      text_content = ''\n",
        "      for p in paragraphs:\n",
        "          text_content += p.get_text() + ' '\n",
        "      scraped_texts.append(text_content.strip())\n",
        "\n",
        "  print(faculty + \": Text successfully scraped from all URLs.\")\n",
        "  # Display the first 200 characters of the first scraped text to verify\n",
        "  print(f\"First scraped text (excerpt): {scraped_texts[0][:200]}...\")\n",
        "  text = ' '.join(scraped_texts)\n",
        "  words = tokenize.word_tokenize(text)\n",
        "  filtered_words = [w for w in words if w not in stopwords.words(\"english\")]\n",
        "  stopwords_custom = ['University', 'Cyprus', 'Professor', 'Research', 'research',\n",
        "                      'et','al', 'two', 'Promponas', 'PhD','BRL','award','Louca', 'grant','non',\n",
        "                      'Athens', 'Greece', 'School', 'aim']\n",
        "  filtered_words = [w for w in filtered_words if w not in stopwords_custom]\n",
        "  filtered_words = [w for w in filtered_words if len(w)>2]\n",
        "  lemmatizer = stem.WordNetLemmatizer()\n",
        "\n",
        "  lem_text1 = \"\"\n",
        "  for word in filtered_words:\n",
        "      lemma = lemmatizer.lemmatize(word.lower())\n",
        "      if len(lemma) < 3:\n",
        "        print(lemma,end=' ')\n",
        "        continue\n",
        "      lem_text1 += lemma + \" \"\n",
        "\n",
        "  from wordcloud import WordCloud\n",
        "  outname = 'BERC'+'_'+faculty+'_wordcloud.jpg'\n",
        "\n",
        "  wc1 = WordCloud(background_color=\"white\", width=600, height=400, min_font_size=15)\n",
        "  wc1.generate(lem_text1)\n",
        "  wc1.to_file(outname)"
      ],
      "metadata": {
        "id": "bjVz518UHXbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61a8fea5-0d02-4114-ddb0-8bcc4b6b738a"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fpas: Text successfully scraped from all URLs.\n",
            "First scraped text (excerpt): Associate Professor Agapios Agapiou received his Diploma and PhD in Chemical Engineering from the National Technical University of Athens (NTUA, Greece) in 2001 and 2006, respectively. Since 2001, he ...\n",
            "us ac feng: Text successfully scraped from all URLs.\n",
            "First scraped text (excerpt): ROBOTIC REHABILITATION  Loucas S. Louca received his Diploma in Mechanical Engineering from the National Technical University of Athens, Greece, in 1992.Â  He then moved to the University of Michigan w...\n",
            "ac mc mc med: Text successfully scraped from all URLs.\n",
            "First scraped text (excerpt): LABORATORY OF  MEDICAL STATISTICS,  EPIDEMIOLOGY &  PUBLIC HEALTH  Dr Nikolopoulos is Associate Professor of Epidemiology and Public Health at the Medical School of the University of Cyprus. He is a g...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Package the generated images in a compressed file for downloading\n",
        "from zipfile import ZipFile\n",
        "with ZipFile('figures.zip', 'w') as myzip:\n",
        "    myzip.write('BERC_fpas_wordcloud.jpg')\n",
        "    myzip.write('BERC_feng_wordcloud.jpg')\n",
        "    myzip.write('BERC_med_wordcloud.jpg')\n",
        "    #myzip.write('BERC_all_wordcloud.jpg')"
      ],
      "metadata": {
        "id": "gBeq3D_PG3xd"
      },
      "execution_count": 84,
      "outputs": []
    }
  ]
}